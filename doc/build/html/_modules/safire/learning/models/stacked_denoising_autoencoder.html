<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>safire.learning.models.stacked_denoising_autoencoder &mdash; Safire 0.0.1r2 documentation</title>
    
    <link rel="stylesheet" href="../../../../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     '0.0.1r2',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <link rel="top" title="Safire 0.0.1r2 documentation" href="../../../../index.html" />
    <link rel="up" title="Module code" href="../../../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../index.html">Safire 0.0.1r2 documentation</a> &raquo;</li>
          <li><a href="../../../index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <h1>Source code for safire.learning.models.stacked_denoising_autoencoder</h1><div class="highlight"><pre>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">.. module:: </span>
<span class="sd">    :platform: Unix</span>
<span class="sd">    :synopsis: ???</span>

<span class="sd">.. moduleauthor: Jan Hajic &lt;hajicj@gmail.com&gt;</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">TT</span>

<span class="kn">from</span> <span class="nn">safire.learning.models.base_supervised_model</span> <span class="kn">import</span> <span class="n">BaseSupervisedModel</span>
<span class="kn">from</span> <span class="nn">safire.learning.models.logistic_regression</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">safire.learning.models.hidden_layer</span> <span class="kn">import</span> <span class="n">HiddenLayer</span>
<span class="kn">from</span> <span class="nn">safire.learning.models.denoising_autoencoder</span> <span class="kn">import</span> <span class="n">DenoisingAutoencoder</span>
<span class="kn">from</span> <span class="nn">safire.learning.interfaces.pretrained_supervised_model_handle</span> <span class="kn">import</span> <span class="n">PretrainedSupervisedModelHandle</span>

<div class="viewcode-block" id="StackedDenoisingAutoencoder"><a class="viewcode-back" href="../../../../safire.learning.models.stacked_denoising_autoencoder.html#safire.learning.models.stacked_denoising_autoencoder.StackedDenoisingAutoencoder">[docs]</a><span class="k">class</span> <span class="nc">StackedDenoisingAutoencoder</span><span class="p">(</span><span class="n">BaseSupervisedModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is the base class for supervised models that</span>
<span class="sd">    can be pre-trained in an unsupervised fashion.</span>
<span class="sd">    </span>
<span class="sd">    The model has two aspects: one is a supervised aspect for</span>
<span class="sd">    classification, the other is an unsupervised aspect for pre-training</span>
<span class="sd">    the weights on unsupervised data so that the model better captures</span>
<span class="sd">    the structure of the data - it should focus on features that actually</span>
<span class="sd">    also describe the data well, not just that they are somehow determined</span>
<span class="sd">    to be useful for classification.</span>
<span class="sd">    </span>
<span class="sd">    This duality is achieved by having two stacks of layers that share</span>
<span class="sd">    weights. The first stack is for pre-training, the second stack is for</span>
<span class="sd">    the supervised (classification) task. The unsupervised layers have inputs</span>
<span class="sd">    from the previous feedforward layer.</span>
<span class="sd">    </span>
<span class="sd">    In the base version, the supervised portion is a classical multilayer</span>
<span class="sd">    perceptron with a logistic regression layer at the end. The unsupervised</span>
<span class="sd">    portion is composed of ``DenoisingAutoencoder``s.</span>
<span class="sd">    </span>
<span class="sd">    On :func:`setup`, the returned handle is a</span>
<span class="sd">    :class:`PretrainedSupervisedModelHandle` instance that as an extra field</span>
<span class="sd">    contains the functions for layer-wise pre-training.</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">n_hidden_list</span><span class="p">,</span>
                 <span class="n">supervised_stack_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">unsupervised_stack_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">logistic_regression_params</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize a Stacked Denoising Autoencoder (SDAE).</span>
<span class="sd">        </span>
<span class="sd">        :type rng: numpy.random.RandomState</span>
<span class="sd">        :param rng: a random number generator used to initialize weights</span>

<span class="sd">        :type inputs: thenao.tensor.TensorType</span>
<span class="sd">        :param inputs: symbol variable that describes the input of</span>
<span class="sd">        the architecture (one minibatch)</span>

<span class="sd">        :type n_in: int</span>
<span class="sd">        :param n_in: number of input units, the dimension of the space</span>
<span class="sd">        in which the datapoints lie</span>

<span class="sd">        :type n_out: int</span>
<span class="sd">        :param n_out: number of output units - think number of classes,</span>
<span class="sd">        because the output units are argmaxed over to produce</span>
<span class="sd">        a prediction for a data point.</span>
<span class="sd">        </span>
<span class="sd">        :type n_layers: int</span>
<span class="sd">        :param n_layers: The number of hidden layers of the model. The</span>
<span class="sd">        logistic regression layer for the supervised stack is added on</span>
<span class="sd">        top of this, so there is ``n_layers + 1`` supervised layers in</span>
<span class="sd">        the end.</span>
<span class="sd">        </span>
<span class="sd">        :type n_hidden_list: list(int)</span>
<span class="sd">        :param n_hidden_list: A list of the hidden layer sizes. The</span>
<span class="sd">        logistic regression layer has then input size ``n_hidden_list[-1]``</span>
<span class="sd">        and output size ``n_out``.</span>
<span class="sd">        </span>
<span class="sd">        :type supervised_stack_params: list(dict)</span>
<span class="sd">        :param supervised_stack_params: A list of dictionaries that will</span>
<span class="sd">        be passed to the corresponding layer in the supervised stack as</span>
<span class="sd">        keyword arguments. </span>
<span class="sd">        </span>
<span class="sd">        :type unsupervised_stack_params: list(dict)</span>
<span class="sd">        :param unsupervised_stack_params: A list of dictionaries that will</span>
<span class="sd">        be passed to the corresponding layer in the unsupervised stack as</span>
<span class="sd">        keyword arguments. </span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c"># Sanity checks</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_hidden_list</span><span class="p">))</span>
        <span class="c"># Generate empty parameter dictionaries</span>
        <span class="k">if</span> <span class="n">supervised_stack_params</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">supervised_stack_params</span> <span class="o">=</span> <span class="p">[</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">unsupervised_stack_params</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">unsupervised_stack_params</span> <span class="o">=</span> <span class="p">[</span> <span class="p">{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">logistic_regression_params</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">logistic_regression_params</span> <span class="o">=</span> <span class="p">{}</span>
            
        <span class="k">assert</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">supervised_stack_params</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">unsupervised_stack_params</span><span class="p">))</span>
        
        <span class="nb">super</span><span class="p">(</span><span class="n">StackedDenoisingAutoencoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
                                                          <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
    
        <span class="c"># Initialize intermediate variables for stack constructions</span>
        <span class="n">input_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_in</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_hidden_list</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">output_sizes</span> <span class="o">=</span> <span class="n">n_hidden_list</span>
        <span class="n">layer_shapes</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_sizes</span><span class="p">,</span> <span class="n">output_sizes</span><span class="p">)</span>      
        
        <span class="c"># Initialize supervised stack</span>
        <span class="n">prev_output</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">l_index</span><span class="p">,</span> <span class="n">l_shape</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_shapes</span><span class="p">):</span>
            <span class="c"># Initialize hidden layer</span>
            <span class="n">current_layer</span> <span class="o">=</span> <span class="n">HiddenLayer</span><span class="p">(</span><span class="n">prev_output</span><span class="p">,</span> <span class="n">l_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">l_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="o">**</span><span class="n">supervised_stack_params</span><span class="p">[</span><span class="n">l_index</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_layer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">current_layer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
            <span class="n">prev_output</span> <span class="o">=</span> <span class="n">current_layer</span><span class="o">.</span><span class="n">outputs</span>
    
        <span class="c"># Initialize logistic regression on top of supervised stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logistic_layer</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span>
                                <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
                                <span class="n">n_in</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span>
                                <span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logistic_layer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c"># Initialize unsupervised stack with shared weights. Default</span>
        <span class="c"># models are autoencoders, but that&#39;s just because this class</span>
        <span class="c"># needs to have something inside.</span>
        <span class="n">prev_output</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsupervised_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">l_index</span><span class="p">,</span> <span class="n">l_shape</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_shapes</span><span class="p">):</span>
            <span class="n">current_layer</span> <span class="o">=</span> <span class="n">DenoisingAutoencoder</span><span class="p">(</span><span class="n">prev_output</span><span class="p">,</span> 
                                        <span class="n">l_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">l_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="p">[</span><span class="n">l_index</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                                        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="p">[</span><span class="n">l_index</span><span class="p">]</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
                                        <span class="o">**</span><span class="n">unsupervised_stack_params</span><span class="p">[</span><span class="n">l_index</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unsupervised_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_layer</span><span class="p">)</span>
            <span class="c"># Weights do not get added to layer params. (Alternatively:</span>
            <span class="c"># rewrite layer params as a dictionary?)</span>
        <span class="c">##### self.params.extend([current_layer.b, current_layer.b_prime])</span>
            <span class="c"># ...the previous line caused an error because the fine-tuning</span>
            <span class="c"># cost doesn&#39;t include the unsupervised layer params</span>
            <span class="n">prev_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">supervised_stack</span><span class="p">[</span><span class="n">l_index</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logistic_layer</span><span class="o">.</span><span class="n">outputs</span>
        
<div class="viewcode-block" id="StackedDenoisingAutoencoder.error"><a class="viewcode-back" href="../../../../safire.learning.models.stacked_denoising_autoencoder.html#safire.learning.models.stacked_denoising_autoencoder.StackedDenoisingAutoencoder.error">[docs]</a>    <span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the proportion of incorrectly classified instances.</span>
<span class="sd">        </span>
<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: Corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label.</span>
<span class="sd">        </span>
<span class="sd">        :returns: The proportion of incorrectly classified instances.</span>
<span class="sd">        </span>
<span class="sd">        :raises: TypeError</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">logistic_layer</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
       </div>
    <span class="k">def</span> <span class="nf">_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the cost expression, binding the response variable for y.</span>
<span class="sd">        Used during setup. The cost used in the combined model is the cost</span>
<span class="sd">        of the supervised phase (fine-tuning the model).</span>
<span class="sd">        </span>
<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: Corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label.</span>
<span class="sd">        </span>
<span class="sd">        :returns: The logistic layer ``_cost`` symbolic expression bound to y.</span>
<span class="sd">           </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supervised_cost</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_supervised_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the cost expression, binding the response variable for y.</span>
<span class="sd">        Used during setup. The cost used in the combined model is the cost</span>
<span class="sd">        of the supervised phase (fine-tuning the model).</span>
<span class="sd">        </span>
<span class="sd">        :type y: theano.tensor.TensorType</span>
<span class="sd">        :param y: Corresponds to a vector that gives for each example the</span>
<span class="sd">                  correct label.</span>
<span class="sd">        </span>
<span class="sd">        :returns: The logistic layer ``_cost`` symbolic expression bound to y.</span>
<span class="sd">           </span>
<span class="sd">        &quot;&quot;&quot;</span>    
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">logistic_layer</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_generate_pretraining_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span>
                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepares for each unsupervised layer the corresponding pre-training</span>
<span class="sd">        Theano function on the given data (X). This function is called during</span>
<span class="sd">        :func:`setup` and its outputs are appended to the resulting </span>
<span class="sd">        :class:`PretrainingModelHandle`.</span>
<span class="sd">        </span>
<span class="sd">        :type X: theano.tensor.TensorType</span>
<span class="sd">        :param X: A theano symbolic variable that represents the input matrix.</span>
<span class="sd">        ``None`` by default (which creates a new variable).</span>
<span class="sd">        </span>
<span class="sd">        :type dataset: Dataset</span>
<span class="sd">        :param dataset: The dataset on which the pre-training should take place.</span>
<span class="sd">        </span>
<span class="sd">        :rtype: list(theano.function)</span>
<span class="sd">        :returns: A list of the compiled pretraining functions, one for each</span>
<span class="sd">        unsupervised layer (index 0 for unsupervised layer with index 0, etc.)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">TT</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">X</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">TT</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">&#39;X&#39;</span><span class="p">)</span>
        
        <span class="n">pretraining_functions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsupervised_stack</span><span class="p">:</span>
            <span class="n">bound_cost</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> 
                                  <span class="c"># We need this X to be the input &#39;processed&#39;</span>
                                  <span class="c"># by the network up to this layer.</span>
                    <span class="c"># FIXME: this solution relies on the fact that the X</span>
                    <span class="c"># that gets passed to this function is at the same time</span>
                    <span class="c"># the symbolic variable to which the model&#39;s input is</span>
                    <span class="c"># bound.</span>
            <span class="n">updates</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_training_updates</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">bound_cost</span><span class="p">,</span>
                                              <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="n">pretraining_fn</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span>
                                <span class="n">outputs</span> <span class="o">=</span> <span class="n">bound_cost</span><span class="p">,</span>
                                <span class="n">updates</span> <span class="o">=</span> <span class="n">updates</span><span class="p">,</span>
                                <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="n">X</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">train_X_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> 
                                                          <span class="n">batch_size</span><span class="p">)</span>
                                <span class="p">})</span>
            <span class="n">pretraining_functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pretraining_fn</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">pretraining_functions</span>
    
    <span class="c"># TODO: </span>
    <span class="c"># Enable setup() to get a model instance; if an instance is given, do not</span>
    <span class="c"># initialize it?</span>
    <span class="c">#  - This would be useful to enable split of setup()</span>
    <span class="c">#    and pretraining_setup()</span>
    <span class="c">#  - This would make it possible to use the same model with already</span>
    <span class="c">#    learned weights for new tasks!!! (on new datasets, etc.)</span>
    <span class="c">#  - This would also necessitate that model handles share model instances</span>
    <span class="c">#     - Good thing? Bad thing?</span>
    <span class="c">#     - ...a step towards multi-threading? Multiple SGDs running</span>
    <span class="c">#       on different parts of a dataset, each from a different</span>
    <span class="c">#       model handle?</span>
    <span class="c">#     - Python should take care of not deleting the instances on handle</span>
    <span class="c">#       destruction...</span>
    <span class="c"># Shouldn&#39;t the setup get a handle instead of a model instance and extract</span>
    <span class="c"># the model instance from the handle? This would protect us from passing </span>
    <span class="c"># bare models around...</span>
    <span class="c">#  - would such bare models be a bad thing? Handles should serve as access</span>
    <span class="c">#    points to model training/validation/testing, but we may want to</span>
    <span class="c">#    use models separately: for printing, loading &amp; saving, etc.; a model</span>
    <span class="c">#    is a fully functioning entity.</span>
    <span class="c"># Separate setup() and pretraining_setup()</span>
    <span class="nd">@classmethod</span>
<div class="viewcode-block" id="StackedDenoisingAutoencoder.setup"><a class="viewcode-back" href="../../../../safire.learning.models.stacked_denoising_autoencoder.html#safire.learning.models.stacked_denoising_autoencoder.StackedDenoisingAutoencoder.setup">[docs]</a>    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.13</span><span class="p">,</span> 
              <span class="n">data_pretrain</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_size_pretrain</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
              <span class="n">learning_rate_pretrain</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">model_init_kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Prepares the train_model, validate_model, test_model and</span>
<span class="sd">        pretrain_model functions on the given dataset and with the given </span>
<span class="sd">        parameters.</span>

<span class="sd">        It is a CLASS METHOD, which during its run actually creates</span>
<span class="sd">        an instance of the model. It is called as </span>

<span class="sd">            &gt;&gt;&gt; model_handle = ModelClass.setup(dataset, params...)</span>
<span class="sd">            </span>
<span class="sd">        Note that the pretraining functions do not necessarily have to be</span>
<span class="sd">        obtained from this class method: the ``model_handle.model_instance``</span>
<span class="sd">        attribute of the returned handle supplies its own method to build</span>
<span class="sd">        the pretraining functions (and this ``setup`` uses exactly this</span>
<span class="sd">        method to build them).</span>

<span class="sd">        :type data: SupervisedDataset</span>
<span class="sd">        :param data: The dataset on which the model will be run.</span>
<span class="sd">        </span>
<span class="sd">        :type model: StackedDenoisingAutoencoder</span>
<span class="sd">        :param model: A model instance that the setup should use.</span>

<span class="sd">        :type batch_size: int</span>
<span class="sd">        :param batch_size: How many data items will be in one minibatch</span>
<span class="sd">                           (the data is split to minibatches for training,</span>
<span class="sd">                           validation and testing)</span>
<span class="sd">                           </span>
<span class="sd">        :type learning_rate: float</span>
<span class="sd">        :param learning_rate: A coefficient that says how much we should move</span>
<span class="sd">                              in the direction of the gradient during SGD</span>
<span class="sd">        </span>
<span class="sd">        :type data_pretrain: UnsupervisedDataset</span>
<span class="sd">        :param data_pretrain: The dataset on which you want to initialize</span>
<span class="sd">                              pre-training (often there is a good reason</span>
<span class="sd">                              to do this: there is a bigger unsupervised</span>
<span class="sd">                              dataset available for pre-training than</span>
<span class="sd">                              a supervised one for fine-tuning). If the</span>
<span class="sd">                              parameter is set to ``None`` (default behavior),</span>
<span class="sd">                              it is linked to the parameter ``data``.</span>

<span class="sd">        :type batch_size_pretrain: int</span>
<span class="sd">        :param batch_size: How many data items will be in one minibatch</span>
<span class="sd">                           when pre-training.</span>
<span class="sd">                           </span>
<span class="sd">        :type learning_rate_pretrain: float</span>
<span class="sd">        :param learning_rate: A coefficient that says how much we should move</span>
<span class="sd">                              in the direction of the gradient during SGD</span>
<span class="sd">                              in pre-training.</span>

<span class="sd">                              </span>
<span class="sd">        :type model_init_kwargs: kwargs</span>
<span class="sd">        :param model_init_kwargs: Various keyword arguments that get passed</span>
<span class="sd">                                  to the model constructor. See constructor</span>
<span class="sd">                                  argument documentation.</span>
<span class="sd">                                     </span>
<span class="sd">        :rtype: PretrainingModelHandle</span>
<span class="sd">        :returns: ``PretrainingModelHandle(model, train_f, validate_f, test_f,</span>
<span class="sd">                  pretraining_fs)``</span>
<span class="sd">                  where &#39;model&#39; is the Model instance initialized during</span>
<span class="sd">                  :func:`setup` and the ``_f`` variables are compiled</span>
<span class="sd">                  theano.functions to use in a learner.                            </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">TT</span><span class="o">.</span><span class="n">lscalar</span><span class="p">()</span> <span class="c"># index of minibatch</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">TT</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">&#39;X&#39;</span><span class="p">)</span>   <span class="c"># data as a matrix</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">TT</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s">&#39;y&#39;</span><span class="p">)</span>  <span class="c"># labels as ints (see data.loader.as_shared)</span>

        <span class="c"># Check for kwargs ... obsolete?</span>
<span class="c">#        cls._check_init_args(model_init_kwargs)</span>

        <span class="c"># Construct the model instance, or use supplied and do sanity checks.</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">cls</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">n_in</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> 
                        <span class="o">**</span><span class="n">model_init_kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c"># Sanity (dimensionality...) checks: </span>
            <span class="c"># - Are we passing a model of the same type as we&#39;re trying</span>
            <span class="c">#   to set up?</span>
            <span class="c"># - Are we passing a dataset that the model can work on?</span>
            <span class="k">print</span> <span class="n">cls</span><span class="p">,</span> <span class="n">model</span>
            <span class="k">assert</span> <span class="n">cls</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">n_in</span> <span class="o">==</span> <span class="n">data</span><span class="o">.</span><span class="n">n_in</span>
            <span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">n_out</span> <span class="o">==</span> <span class="n">data</span><span class="o">.</span><span class="n">n_out</span>

        <span class="c"># The cost to minimize during training: negative log-likelihood</span>
        <span class="c"># of the training data (symbolic)</span>
        <span class="n">bound_cost</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_cost</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c"># Specify how to update the parameters of the model as a list of</span>
        <span class="c"># (variable, update expression) pairs.</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_training_updates</span><span class="p">(</span><span class="n">cost</span><span class="o">=</span><span class="n">bound_cost</span><span class="p">,</span> 
                                          <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="c"># Compile a Theano function that trains the model: returns the cost</span>
        <span class="c"># (negative log-likelihood) and updates the model parameters based</span>
        <span class="c"># on the rules defined in updates.</span>
        <span class="n">train_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">bound_cost</span><span class="p">,</span>
                <span class="n">updates</span> <span class="o">=</span> <span class="n">updates</span><span class="p">,</span>
                <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">X</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">train_X_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span>
                    <span class="n">y</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">train_y_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
                <span class="p">})</span>

        <span class="c"># Compile a Theano function that computes the mistakes that are made</span>
        <span class="c"># by the model on a minibatch of devel/test data</span>
        <span class="n">validate_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
            <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">X</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">devel_X_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span>
                <span class="n">y</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">devel_y_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="p">})</span>

        <span class="n">test_model</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
            <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">X</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">test_X_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span>
                <span class="n">y</span><span class="p">:</span> <span class="n">data</span><span class="o">.</span><span class="n">test_y_batch</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="p">})</span>
        
        
        <span class="k">if</span> <span class="n">data_pretrain</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">data_pretrain</span> <span class="o">=</span> <span class="n">data</span>
            
        <span class="n">pretraining_functions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_generate_pretraining_functions</span><span class="p">(</span>
                                                                <span class="n">data_pretrain</span><span class="p">,</span>
                                                                <span class="n">X</span><span class="p">,</span>
                                                  <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                                     <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">PretrainedSupervisedModelHandle</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_model</span><span class="p">,</span> 
                                               <span class="n">validate_model</span><span class="p">,</span> <span class="n">test_model</span><span class="p">,</span> 
                                               <span class="n">pretraining_functions</span><span class="p">)</span>
</pre></div></div></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../index.html">Safire 0.0.1r2 documentation</a> &raquo;</li>
          <li><a href="../../../index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2014, Jan Hajic jr..
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>