[_info]
name='test_retrieval_config'

[_loader]
_class=safire.data.loaders.MultimodalShardedDatasetLoader
root='test-data/'
name='test-data'

[_builder]
_import=safire.datasets.dataset.Dataset,safire.utils.transcorp,safire.data.composite_corpus.CompositeCorpus
save=True
save_every=False
savename=_loader.pipeline_name(_info.name)
log_blocks=False
log_saving=False

[_assembly]
# Text processing
_vtcorp_=vtcorp
_vtcorp_serialized_=vtcorp_serializer[_vtcorp_]
_w2v_=w2v[_vtcorp_serialized_]
_w2v_nonempty_=w2v_filter_empty[_w2v_]
_w2v_tanh_=w2v_tanh[_w2v_nonempty_]
_w2v_serialized_=w2v_serializer[_w2v_tanh_]
# Image processing
_icorp_=icorp
_tanh_icorp_=tanh[_icorp_]
_img_serialized_=i_serializer[_tanh_icorp_]
# Combining data for the joint model
_combined_=CompositeCorpus((_w2v_serialized_, _img_serialized_), names=('txt', 'img'), aligned=False)
_joint_data_=flatten[_combined_]
_joint_serialized_=joint_serializer[_joint_data_]
# The trained model is never applied, only used to initialize the t2i handle.
# t2i
_query_vtcorp_=vtcorp
_query_w2v_=w2v[_query_vtcorp_]
_query_w2v_nonempty_=w2v_filter_empty[_query_w2v_]
_query_tanh_=tanh[_query_w2v_nonempty_]
_t2i_=t2i_sftrans[_query_tanh_]
_t2i_serialized_=q_serializer[_t2i_]
# Retrieval
_similarities_=similarity[_t2i_serialized_]
_aggregated_similarities_=aggregator[_similarities_]
# Introspection (documents and retrieved images)
_intro_vtcorp_=doc_vtcorp
_intro_combined_=CompositeCorpus((_intro_vtcorp_, _aggregated_similarities_), names=('txt', 'img'), aligned=True)
_intro_flattened_=intro_flatten[_intro_combined_]
_intro_results_=t2i_introspection[_intro_flattened_]

[_persistence]
loader=_loader
_img_serialized_=image_source
_w2v_serialized_=w2v_source
_joint_serialized_=joint_w2v_and_img
_t2i_=test_t2i
_similarities_=test_similarities

############################## Introspection #################################
[t2i_introspection]
_class=safire.introspection.interfaces.IntrospectionTransformer
_dependencies=retrieval_writer
_access_deps=retrieval_writer:introspection.writer
writer=retrieval_writer
corpus=_intro_flattened_

[intro_flatten]
_import=safire.utils.transcorp.compute_docname_flatten_mapping
_class=safire.datasets.transformations.FlattenComposite
composite=_intro_combined_
structured=True

[doc_vtcorp]
_class=safire.data.vtextcorpus.VTextCorpus
_dependencies=tokenfilter
_access_deps=tokenfilter:vtcorp.tokenfilter
input_root=_loader.root
input=os.path.join(_loader.root, _loader.layout.vtlist)
token_filter=tokenfilter
pfilter=0.3
pfilter_full_freqs=True
filter_capital=True
precompute_vtlist=True
tokens=False

[text_writer]
_class=safire.introspection.writers.HtmlVocabularyWriter
root=_loader.root
prefix=_loader.layout.name + '.documents'
top_k=100
min_freq=0.001

[ret_img_writer]
_class=safire.introspection.writers.HtmlSimilarImagesWriter
root=os.path.join(_loader.root, _loader.layout.img_dir)
image_id2doc=safire.utils.transcorp.get_id2doc_obj(_img_serialized_)

[retrieval_writer]
_class=safire.introspection.writers.HtmlStructuredFlattenedWriter
_dependencies=text_writer,ret_img_writer
_access_deps=text_writer:retrieval_writer.writers[0]|ret_img_writer:retrieval_writer.writers[1]
root=_loader.root
writers=(text_writer, ret_img_writer)

###################### Combining text and image data #########################
[flatten]
_import=safire.utils.transcorp.compute_docname_flatten_mapping
_class=safire.datasets.transformations.FlattenComposite
composite=_combined_
structured=False
indexes=compute_docname_flatten_mapping(_combined_, os.path.join(_loader.root, _loader.layout.textdoc2imdoc))

[joint_serializer]
_import=safire.data.sharded_corpus.ShardedCorpus
_class=safire.data.serializer.Serializer
corpus=_joint_data_
serializer_class=ShardedCorpus
fname=_loader.pipeline_serialization_target('.joint_data')
overwrite=False

########################## Training the t2i model ############################
[sftrans]
_class=safire.learning.interfaces.safire_transformer.SafireTransformer
_dependencies=model_handle,learner
_access_deps=model_handle:sftrans.model_handle
_import=safire.datasets.dataset.Dataset
model_handle=model_handle
dataset=Dataset(_joint_serialized_)
learner=learner
dense_throughput=False

[model_handle]
_import=safire.learning.models.denoising_autoencoder.DenoisingAutoencoder,theano
_init=DenoisingAutoencoder.setup
data=Dataset(_joint_serialized_)
n_out=200
activation=theano.tensor.nnet.sigmoid
backward_activation=theano.tensor.tanh
reconstruction='mse'
heavy_debug=False

[learner]
_class=safire.learning.learners.base_sgd_learner.BaseSGDLearner
n_epochs=0
b_size=10
learning_rate=0.13
validation_frequency=4
plot_transformation=False

[t2i_handle]
_import=safire.learning.interfaces.model_handle.MultimodalClampedSamplerModelHandle
_dependencies=model_handle
_init=MultimodalClampedSamplerModelHandle.clone
handle=model_handle
dim_text=safire.utils.transcorp.dimension(safire.utils.transcorp.get_composite_source(_joint_serialized_, 'txt'))
dim_img=safire.utils.transcorp.dimension(safire.utils.transcorp.get_composite_source(_joint_serialized_, 'img'))
k=10
sample_visible=False

[t2i_sftrans]
_class=safire.learning.interfaces.safire_transformer.SafireTransformer
_dependencies=t2i_handle
_access_deps=t2i_handle:t2i_sftrans.model_handle
model_handle=t2i_handle

[q_serializer]
_import=safire.data.sharded_corpus.ShardedCorpus
_class=safire.data.serializer.Serializer
corpus=_t2i_
serializer_class=ShardedCorpus
fname=_loader.pipeline_serialization_target('.query_t2i_data')
overwrite=False

##############################################################################
[similarity]
_class=safire.utils.transformers.SimilarityTransformer
_dependencies=index
_access_deps=index:similarity.index
index=index

[index]
_class=gensim.similarities.Similarity
_dependencies=iloader
output_prefix=iloader.output_prefix('.img')
corpus=_img_serialized_
num_features=safire.utils.transcorp.dimension(_img_serialized_)
num_best=10

[iloader]
_class=safire.data.loaders.IndexLoader
root=_loader.root
name=_loader.layout.name

[aggregator]
_class=safire.utils.transformers.ItemAggregationTransform
average=True

############# Image inputs #############
[icorp]
_class=safire.data.imagenetcorpus.ImagenetCorpus
input=os.path.join(_loader.root, _loader.layout.image_vectors)
delimiter=';'
dim=4096

[tanh]
_import=numpy
_class=safire.utils.transformers.GeneralFunctionTransform
fn=numpy.tanh
multiplicative_coef=0.4

[i_serializer]
_import=safire.data.sharded_corpus.ShardedCorpus
_class=safire.data.serializer.Serializer
corpus=_tanh_icorp_
serializer_class=ShardedCorpus
fname=_loader.pipeline_serialization_target('image_source')
overwrite=True

############## Text inputs ##############
[w2v_serializer]
_import=safire.data.sharded_corpus.ShardedCorpus
_class=safire.data.serializer.Serializer
corpus=_w2v_tanh_
serializer_class=ShardedCorpus
fname=_loader.pipeline_serialization_target('.w2v_data')
overwrite=True

[w2v_tanh]
_import=numpy
_class=safire.utils.transformers.GeneralFunctionTransform
fn=numpy.tanh
multiplicative_coef=0.4

[w2v_filter_empty]
_import=safire.data.filters.frequency_filters
_class=safire.data.document_filter.DocumentFilterTransform
flt=safire.data.filters.frequency_filters.zero_length_filter

[w2v]
_class=safire.data.word2vec_transformer.Word2VecTransformer
embeddings=os.path.join('C:/', 'Users', 'Lenovo', 'word2vec', 'ces_wiki.edict.pkl')
id2word=safire.utils.transcorp.get_id2word_obj(_vtcorp_)

[vtcorp_serializer]
_import=safire.data.sharded_corpus.ShardedCorpus
_class=safire.data.serializer.Serializer
corpus=_vtcorp_
serializer_class=ShardedCorpus
fname=_loader.pipeline_serialization_target('.text_data')
overwrite=True
gensim_serialization=True
gensim_retrieval=True

[tokenfilter]
_class=safire.data.filters.positionaltagfilter.PositionalTagTokenFilter
values=['N', 'A', 'V']
t_position=0

[vtcorp]
_class=safire.data.vtextcorpus.VTextCorpus
_dependencies=tokenfilter
_access_deps=tokenfilter:vtcorp.tokenfilter
input_root=_loader.root
input=os.path.join(_loader.root, _loader.layout.vtlist)
token_filter=tokenfilter
pfilter=0.3
pfilter_full_freqs=True
filter_capital=True
precompute_vtlist=False
tokens=True
